===================================================================================================================================================================
REDHAT CEPH ADMINISTRATION:
===================================================================================================================================================================
Lesson-1: OVERVIEW
====================
A Red Hat Ceph Storage cluster is the foundation for all Ceph deployments. After deploying a Red Hat Ceph Storage cluster, 
there are administrative operations for keeping a Red Hat Ceph Storage cluster healthy and performing optimally.

The Red Hat Ceph Storage Administration Guide helps storage administrators to perform such tasks as:

- How do I check the health of my Red Hat Ceph Storage cluster?
- How do I start and stop the Red Hat Ceph Storage cluster services?
- How do I add or remove an OSD from a running Red Hat Ceph Storage cluster?
- How do I manage user authentication and access controls to the objects stored in a Red Hat Ceph Storage cluster?
- I want to understand how to use overrides with a Red Hat Ceph Storage cluster.
- I want to monitor the performance of the Red Hat Ceph Storage cluster. A basic Ceph storage cluster consist of two types of daemons:
- A Ceph Object Storage Device (OSD) stores data as objects within placement groups assigned to the OSD
- A Ceph Monitor maintains a master copy of the cluster map
- A production system will have three or more Ceph Monitors for high availability and typically a minimum of 50 OSDs for acceptable load balancing,
  data re-balancing and data recovery.
==========================================================================================================================================================================================================================
Lesson-2:UNDERSTANDING PROCESS MANAGEMENT FOR CEPH:
====================================================
CEPH PROCESS MANAGEMENT:
------------------------
In Red Hat Ceph Storage, all process management is done through the Systemd service. 
Each time you want to start, restart, and stop the Ceph daemons, you must specify the daemon type or the daemon instance

2.1.STARTING, STOPPING, AND RESTARTING ALL CEPH DAEMONS
---------------------------------------------------------
Start, stop, and restart all Ceph daemons as an admin from the node.

systemctl start ceph.target
systemctl stop ceph.target
systemctl restart ceph.target

2.2.STARTING, STOPPING, AND RESTARTING THE CEPH DAEMONS BY TYPE:
----------------------------------------------------------------
On Ceph Monitor nodes:

systemctl start ceph-mon.target
systemctl stop ceph-mon.target
systemctl restart ceph-mon.target

On Ceph Manager nodes:
 
systemct start ceph-mgr.target
systemctl stop ceph-mgr.target
systemctl restart ceph-mgr.target

On Ceph OSD nodes: Starting:

systemctl start ceph-osd.target
systemctl stop ceph-osd.target
systemctl restart ceph-osd.target

On Ceph Object Gateway nodes:

systemctl start ceph-radosgw.target
systemctl stop ceph-radosgw.target
systemctl restart ceph-radosgw.target

2.3.STARTING, STOPPING, AND RESTARTING THE CEPH DAEMONS BY INSTANCE:
---------------------------------------------------------------------

On a Ceph Monitor node:

systemctl start ceph-mon@MONITOR_HOST_NAME 
systemctl stop ceph-mon@MONITOR_HOST_NAME
systemctl restart ceph-mon@MONITOR_HOST_NAME

On a Ceph Manager node:

systemctl start ceph-mgr@MANAGER_HOST_NAME
systemctl stop ceph-mgr@MANAGER_HOST_NAME 
systemctl restart ceph-mgr@MANAGER_HOST_NAME

On a Ceph OSD node:

systemctl start ceph-osd@OSD_NUMBER 
systemctl stop ceph-osd@OSD_NUMBER 
systemctl restart ceph-osd@OSD_NUMBER 

Replace:
---------
OSD_NUMBER with the ID number of the Ceph OSD.
For example, when looking at the ceph osd tree command output, osd.0 has an ID of 0.

On a Ceph Object Gateway node:

systemctl start ceph-radosgw@rgw.OBJ_GATEWAY_HOST_NAME
systemctl stop ceph-radosgw@rgw.OBJ_GATEWAY_HOST_NAME 
systemctl restart ceph-radosgw@rgw.OBJ_GATEWAY_HOST_NAME

Replace:
OBJ_GATEWAY_HOST_NAME with the name of the Ceph Object Gateway node.

2.4.POWERING DOWN AND REBOOTING RED HAT CEPH STORAGE CLUSTER:
===============================================================
* Powering down the Red Hat Ceph Storage cluster:
-------------------------------------------------
1. StoptheclientsfromusingtheRBDimagesandRADOSGatewayonthisclusterandanyother clients.
2. The cluster must be in healthy state (Health_OK and all PGs active+clean) before proceeding. Run ceph status on a node with the    
   client keyrings, for example, the Ceph Monitor or OpenStack controller nodes, to ensure the cluster is healthy.
3. IfyouusetheCephFileSystem(CephFS),theCephFSclustermustbebroughtdown.Takinga
   CephFS cluster down is done by reducing the number of ranks to flag, and then failing the last rank.

Example:
--------
[root@osd ~]# ceph fs set FS_NAME max_mds 1
[root@osd ~]# ceph mds deactivate FS_NAME:1 # rank 2 of 2 
[root@osd ~]# ceph status # wait for rank 1 to finish stopping 
[root@osd ~]# ceph fs set FS_NAME cluster_down true 
[root@osd ~]# ceph mds fail FS_NAME:0

Setting the cluster_down flag prevents standbys from taking over the failed rank.

4. Setthenoout,norecover,norebalance,nobackfill,nodownandpauseflags.Runthe following on a node with the client keyrings. 
   For example, the Ceph Monitor or OpenStack controller node:

root@mon ~]# ceph osd set noout 
[root@mon ~]# ceph osd set norecover 
[root@mon ~]# ceph osd set norebalance 
[root@mon ~]# ceph osd set nobackfill 
[root@mon ~]# ceph osd set nodown 
[root@mon ~]# ceph osd set pause

5. ShutdowntheOSDnodesonebyone:

[root@osd ~]# systemctl stop ceph-osd.target

6. Shutdownthemonitornodesonebyone:

[root@mon ~]# systemctl stop ceph-mon.target

* Rebooting the Red Hat Ceph Storage cluster:
----------------------------------------------
1. Powerontheadministrationnode.

2. Poweronthemonitornodes:

[root@mon ~]# systemctl start ceph-mon.target

3. PowerontheOSDnodes:

[root@osd ~]# systemctl start ceph-osd.target

4. Waitforallthenodestocomeup.Verifyalltheservicesareupandtheconnectivityisfine between the nodes.

5. Unsetthenoout,norecover,norebalance,nobackfill,nodownandpauseflags.Runthe following on a node with the client keyrings. For 
example, the Ceph Monitor or OpenStack controller node:

[root@mon ~]# ceph osd unset noout 
[root@mon ~]# ceph osd unset norecover 
[root@mon ~]# ceph osd unset norebalance 
[root@mon ~]# ceph osd unset nobackfill 
[root@mon ~]# ceph osd unset nodown 
[root@mon ~]# ceph osd unset pause

6. IfyouusetheCephFileSystem(CephFS),theCephFSclustermustbebroughtbackupby setting the cluster_down flag to false:

[root@admin~]# ceph fs set FS_NAME cluster_down false

7. Verify the cluster is in healthy state (Health_OK and all PGs active+clean). Run ceph status on a node with the client keyrings. 

For example, the Ceph Monitor or OpenStack controller nodes, to ensure the cluster is healthy.

==================================================================================================================================================================
Lesson-3:MONITORING A CEPH STORAGE CLUSTER
===========================================
As a storage administrator, you can monitor the overall health of the Red Hat Ceph Storage cluster, 
along with monitoring the health of the individual components of Ceph.
Ceph storage cluster clients connect to a Ceph Monitor and receive the latest version of the storage cluster map
Ceph OSDs must peer the placement groups on the primary OSD with the copies of the placement groups on secondary OSDs. 
If faults arise, peering will reflect something other than the active + clean state.

3.1. HIGH-LEVEL MONITORING OF A CEPH STORAGE CLUSTER:
------------------------------------------------------
As a storage administrator, you can monitor the health of the Ceph daemons to ensure that they are up and running.

3.1.1. Using the Ceph command interface interactively:
-------------------------------------------------------
You can interactively interface with the Ceph storage cluster by using the ceph command-line utility.

Bare-metaldeployments:
----------------------
[root@mon ~]# ceph 
ceph> health
ceph> status
ceph> quorum_status
ceph> mon_status

Containerdeployments:
-----------------------
docker exec -it ceph-mon-MONITOR_NAME /bin/bash
podman exec -it ceph-mon-mon01 /bin/bash


3.1.2. Checking the storage cluster health:
--------------------------------------------
After you start the Ceph storage cluster, and before you start reading or writing data, check the storage 
cluster’s health first.

1.You can check on the health of the Cephstorage cluster with the following:

[root@mon ~]# ceph health

2.Ifyouspecifiednon-defaultlocationsfortheconfigurationorkeyring,youcanspecifytheir locations:

[root@mon ~]# ceph -c /path/to/conf -k /path/to/keyring health

Upon starting the Ceph cluster, you will likely encounter a health warning such as HEALTH_WARN XXX num 
placement groups stale. Wait a few moments and check it again. When the storage cluster is ready,
cephhealthshouldreturnamessagesuchas HEALTH_OK.Atthatpoint,itisokaytobeginusing the cluster.

3.1.3. Watching storage cluster events:
----------------------------------------
You can watch events that are happening with the Ceph storage cluster using the command-line interface

1. Towatchthecluster’songoingeventsonthecommandline,openanewterminal,andthen enter:

[root@mon ~]# ceph -w

Ceph will print each event. For example, a tiny Ceph cluster consisting of one monitor and two OSDs may 
print the following:

The output provides: Cluster ID
Cluster health status
The monitor map epoch and the status of the monitor quorum
The OSD map epoch and the status of OSDs
The placement group map version
The number of placement groups and pools
The notional amount of data stored and the number of objects stored The total amount of data stored

3.1.4. Understanding the storage clusters usage stats:
--------------------------------------------------------
To check a cluster’s data usage and data distribution among pools, use the df option. It is similar to the Linux df command. Execute the following:

[root@mon ~]# ceph df

3.1.5. Understanding the OSD usage stats:
--------------------------------------------
Use the ceph osd df command to view OSD utilization stats.

[root@mon]# ceph osd df

3.1.6. Checking the Red Hat Ceph Storage cluster status:
----------------------------------------------------------
You can check the status of the Red Hat Ceph Storage cluster from the command-line interface.

1. Tocheckastoragecluster’sstatus,executethefollowing:
[root@mon ~]# ceph status

Or:

[root@mon ~]# ceph -s

2. Ininteractivemode,typestatusandpressEnter: 
[root@mon ~]# ceph> status

3.1.7. Checking the Ceph Monitor status:
------------------------------------------
1. Todisplaythemonitormap,executethefollowing:

[root@mon ~]# ceph mon stat

or [root@mon ~]# ceph mon dump

2. Tocheckthequorumstatusforthestoragecluster,executethefollowing:

[root@mon ~]# ceph quorum_status -f json-pretty

3.1.8. Understanding the Ceph OSD status:
--------------------------------------------
An important aspect of monitoring OSDs is to ensure that when the cluster is up and running that all
OSDs that are in the cluster are up and running, too. To see if all OSDs are running, execute:
[root@mon ~]# ceph osd stat
or
The result should tell you the map epoch, eNNNN, the total number of OSDs, x, how many, y, are up, and how many, z, are in:
eNNNN: x osds: y up, z in
IfthenumberofOSDsthatareintheclusterismorethanthenumberofOSDsthatare up.Executethe following command to identify the ceph-osd daemons that aren’t running:
[root@mon ~]# ceph osd tree
Example
# id weight type name up/down reweight -1 3 pool default
-3 3 rack mainrack
-2 3 host osd-host
[root@mon ~]# ceph osd dump

TIP:
-----
The ability to search through a well-designed CRUSH hierarchy may help you troubleshoot the storage cluster by identifying the physical locations faster.
If an OSD is down, connect to the node and start it. You can use Red Hat Storage Console to restart the OSD node, or you can use the command line.
-------------------------------------------------------------------------------------------------------------------------------------------------------------------
=================================================================================================================================================================================






























































































