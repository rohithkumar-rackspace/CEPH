====================================================================================================================================================================
                                           REDHAT CEPH STORAGE ARCTECTURE
====================================================================================================================================================================
Red Hat Ceph Storage cluster is a distributed data object store designed to provide excellent performance, reliability and scalability. 
Distributed object stores are the future of storage, because they accommodate unstructured data, and because clients can use modern object
interfaces and legacy interfaces simultaneously.

For example:
------------
APIs in many languages (C/C++, Java, Python) 
RESTful interfaces (S3/Swift)
Block device interface
Filesystem interface

At the heart of every Ceph deployment is the Red Hat Ceph Storage cluster. It consists of three types of daemons:

Ceph OSD Daemon: Ceph OSDs store data on behalf of Ceph clients. Additionally, Ceph OSDs utilize the CPU, 
                 memory and networking of Ceph nodes to perform data replication, erasure coding, rebalancing, 
                 recovery, monitoring and reporting functions.
                 
Ceph Monitor: A Ceph Monitor maintains a master copy of the Red Hat Ceph Storage cluster map with the current 
              state of the Red Hat Ceph Storage cluster. Monitors require high consistency, and use Paxos to 
              ensure agreement about the state of the Red Hat Ceph Storage cluster.
              
Ceph Manager: The Ceph Manager maintains detailed information about placement groups, process metadata 
              and host metadata in lieu of the Ceph Monitor—​significantly improving performance at scale. 
              The Ceph Manager handles execution of many of the read-only Ceph CLI queries, such as placement 
              group statistics. The Ceph Manager also provides the RESTful monitoring APIs.
              
Ceph client interfaces read data from and write data to the Red Hat Ceph Storage cluster. Clients need the following data to communicate with the Red Hat Ceph Storage cluster:
- The Ceph configuration file, or the cluster name (usually ceph) and the monitor address 
- The pool name
- The user name and the path to the secret key.

- Ceph clients maintain object IDs and the pool names where they store the objects. 
- To store and retrieve data, Ceph clients access a Ceph Monitor and retrieve the latest copy of the Red Hat Ceph Storage cluster map
- Then, Ceph clients provide an object name and pool name to librados, which computes an object’s placement group and the primary OSD 
  for storing and retrieving data using the CRUSH (Controlled Replication Under Scalable Hashing) algorithm. 
- The Ceph client connects to the primary OSD where it may perform read and write operations. There is no intermediary server, broker 
  or bus between the client and the OSD.

When an OSD stores data, it receives data from a Ceph client—​
whether the client is a Ceph Block Device, 
                      a Ceph Object Gateway,
                      a Ceph Filesystem or another interface—​and it stores the data as an object.

Ceph OSDs store all data as objects in a flat namespace. There are no hierarchies of directories. 
An object has a cluster-wide unique identifier, binary data, and metadata consisting of a set of name/value pairs.

Ceph clients define the semantics for the client’s data format. For example, the Ceph block device maps a block device 
image to a series of objects stored across the cluster.

==============================================================================================================================================================================
CHAPTER 2. THE CORE CEPH COMPONENTS:
=========================================
A Red Hat Ceph Storage cluster can have a large number of Ceph nodes for limitless scalability, high availability and performance. 
Write and read data
Compress data
Ensure durability by replicating or erasure coding data
Monitor and report on cluster health—​also called 'heartbeating' 
Redistribute data dynamically—​also called 'backfilling'
Ensure data integrity; and,
Recover from failures.

2.1. CEPH POOLS:
----------------
The Ceph storage cluster stores data objects in logical partitions called 'Pools.' Ceph administrators can create pools for particular 
types of data, such as for block devices, object gateways, or simply just to separate one group of users from another.

From the perspective of a Ceph client, the storage cluster is very simple. When a Ceph client reads or writes data using an I/O context, 
it always connects to a storage pool in the Ceph storage cluster. The client specifies the pool name, a user and a secret key, so the pool 
appears to act as a logical partition with access controls to its data objects.

Ceph pools define:

Pool Type: he data durability method is pool-wide, 

Placement Groups: 
- Ceph must handle many types of operations, including data durability via replicas or erasure code chunks, data integrity by scrubbing or CRC checks, replication,
- Consequently, managing data on a per-object basis presents a scalability and performance bottleneck. 
- The CRUSH algorithm computes the placement group for storing an object and computes the Acting Set of OSDs for the placement group. 
- CRUSH puts each object into a placement group. 
- Then, CRUSH stores each placement group in a set of OSDs.
- System administrators set the placement group count when creating or modifying a pool.

CRUSH Ruleset:
--------------
- CRUSH plays another important role: 
- CRUSH can detect failure domains and performance domains. 
- CRUSH can identify OSDs by storage media type and organize OSDs hierarchically into nodes, racks, and rows.
- CRUSH enables Ceph OSDs to store object copies across failure domains. 

For example, copies of an object may get stored in different server rooms, aisles, racks and nodes. 
If a large part of a cluster fails, such as a rack, the cluster can still operate in a degraded state 
until the cluster recovers.


2.2. CEPH AUTHENTICATION:
--------------------------
- To identify users and protect against man-in-the-middle attacks, 
- Ceph provides its cephx authentication system, which authenticates users and daemons.
- Cephx uses shared secret keys for authentication, meaning both the client and the monitor 
  cluster have a copy of the client’s secret key. 
- he authentication protocol enables both parties to prove to each other that they have a copy
  of the key without actually revealing it.
- This provides mutual authentication,which means the cluster is sure the user possesses the secret key,
  and the user is sure that the cluster has a copy of the secret key.


Cephx:
------
The cephx authentication protocol operates in a manner similar to Kerberos.

2.3. CEPH PLACEMENT GROUPS:
----------------------------
- Storing millions of objects in a cluster and managing them individually is resource intensive. 
- So Ceph uses placement groups (PGs) to make managing a huge number of objects more efficient.
- A PG is a subset of a pool that serves to contain a collection of objects.
- Ceph shards a pool into a series of PGs. 
- Then, the CRUSH algorithm takes the cluster map and the status of the cluster into account and distributes the 
  PGs evenly and pseudo-randomly to OSDs in the cluster.


- When a system administrator creates a pool, CRUSH creates a user-defined number of PGs for the pool. 
- the number of PGs has a performance impact when Ceph needs to move a PG from one OSD to another OSD. 
- Ceph ensures against data loss by storing replicas of an object or by storing erasure code  chunks of an object. 

- The CRUSH algorithm and PGs make Ceph dynamic. Changes in the cluster map or the cluster state may result in Ceph moving PGs from one OSD to another automatically.

Expanding the Cluster. & An OSD Fails

For Ceph clients, the CRUSH algorithm via librados makes the process of reading and writing objects very simple. A Ceph client simply writes an object to a pool or reads an object from a pool. The primary OSD in the acting set can write replicas of the object or erasure code chunks of the object to the secondary OSDs in the acting set on behalf of the Ceph client.

If the cluster map or cluster state changes, the CRUSH computation for which OSDs store the PG will change too. For example, a Ceph client may write object foo to the pool bar. CRUSH will assign the object to PG 1.a, and store it on OSD 5, which makes replicas on OSD 10 and OSD 15 respectively. If OSD 5 fails, the cluster state changes. When the Ceph client reads object foo from pool bar, the client vialibradoswillautomaticallyretrieveitfrom OSD10asthenewprimaryOSDdynamically.

The Ceph client via librados connects directly to the primary OSD within an acting set when writing and reading objects. Since I/O operations do not use a centralized broker, network oversubscription is typically NOT an issue with Ceph.

2.5. CEPH CRUSH RULESET:
--------------------------









































