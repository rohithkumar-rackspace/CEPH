====================================================================================================================================================================
                                           REDHAT CEPH STORAGE ARCTECTURE
====================================================================================================================================================================
Chapter-1:THE CEPH ARCHITECTURE
================================
1.Introduction:
----------------
Red Hat Ceph Storage cluster is a distributed data object store designed to provide excellent performance, reliability and scalability.
For example:
------------
APIs in many languages (C/C++, Java, Python) 
RESTful interfaces (S3/Swift)
Block device interface
Filesystem interface

At the heart of every Ceph deployment is the Red Hat Ceph Storage cluster. It consists of three types of daemons:

Ceph OSD Daemon: 
----------------
Ceph OSDs store data on behalf of Ceph clients. Additionally, Ceph OSDs utilize the CPU, memory and networking of Ceph nodes to perform data replication, 
erasure coding, rebalancing, recovery, monitoring and reporting functions.

Ceph Monitor: 
--------------
A Ceph Monitor maintains a master copy of the Red Hat Ceph Storage cluster map with the current state of the Red Hat Ceph Storage cluster. 
Monitors require high consistency, and use Paxos to ensure agreement about the state of the Red Hat Ceph Storage cluster.

Ceph Manager:
--------------
The Ceph Manager maintains detailed information about placement groups, process metadata and host metadata in lieu of the Ceph Monitor—​significantly 
improving performance at scale. The Ceph Manager handles execution of many of the read-only Ceph CLI queries, such as placement group statistics.
The Ceph Manager also provides the RESTful monitoring APIs.

Ceph client interfaces read data from and write data to the Red Hat Ceph Storage cluster. 
Clients need the following data to communicate with the Red Hat Ceph Storage cluster:

- The Ceph configuration file, or the cluster name (usually ceph) and the monitor address 
- The pool name
- The user name and the path to the secret key.

Ceph clients maintain object IDs and the pool names where they store the objects. 
However, they do not need to maintain an object-to-OSD index or communicate with a centralized object index to look up object locations. 
To store and retrieve data, Ceph clients access a Ceph Monitor and retrieve the latest copy of the Red Hat Ceph Storage cluster map.
Then, Ceph clients provide an object name and pool name to librados, which computes an object’s placement group and the primary OSD
for storing and retrieving data using the CRUSH (Controlled Replication Under Scalable Hashing) algorithm. The Ceph client connects
to the primary OSD where it may perform read and write operations. There is no intermediary server, broker or bus between the client and the OSD.

When an OSD stores data, it receives data from a Ceph client—​whether the client is a Ceph Block Device, 
a Ceph Object Gateway, a Ceph Filesystem or another interface—​and it stores the data as an object.

Ceph clients define the semantics for the client’s data format. For example, the Ceph block device maps a block device image to a series of
objects stored across the cluster.
===============================================================================================================================================================================================
Chapter 2:THE CORE CEPH COMPONENTS
====================================
A Red Hat Ceph Storage cluster can have a large number of Ceph nodes for limitless scalability, high availability and performance.
Each node leverages non-proprietary hardware and intelligent Ceph daemons that communicate with each other to:

- Write and read data
- Compress data
- Ensure durability by replicating or erasure coding data
- Monitor and report on cluster health—​also called 'heartbeating' 
- Redistribute data dynamically—​also called 'backfilling'
- Ensure data integrity; and,
- Recover from failures.


To the Ceph client interface that reads and writes data, a Red Hat Ceph Storage cluster looks like a simple pool where it stores data.
However, librados and the storage cluster perform many complex operations in a manner that is completely transparent to the client interface.
Ceph clients and Ceph OSDs both use the CRUSH (Controlled Replication Under Scalable Hashing) algorithm. 


2.1: CEPH POOLS:
-----------------
- The Ceph storage cluster stores data objects in logical partitions called 'Pools.' 
- Ceph administrators can create pools for particular types of data, such as for block devices, 
  object gateways, or simply just to separate one group of users from another.
- From the perspective of a Ceph client, the storage cluster is very simple 
- it always connects to a storage pool in the Ceph storage cluster. 
- The client specifies the pool name, a user and a secret key, so the pool appears to act as a logical partition with access controls to its data objects.

Ceph pools define:
-------------------
Pool Type: The data durability method is pool-wide,
---------

Placement Groups:
------------------
- a Ceph pool might store millions of data objects or more. 
- Ceph must handle many types of operations, including data durability via replicas or erasure code chunks, data integrity by scrubbing or CRC checks, replication, 
  rebalancing and recovery. 
- 





















































